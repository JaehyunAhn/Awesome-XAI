# Awesome-XAI

<p align="center"><img width="40%" src="imgs/q_and_a.png" /></p>

## 1. Visualization of CNN representations

### 1-1. Gradient-based method
Compute gradient of the score or intermediate neuron with respect to the input image.

* Feature Visualization [\[link\]] (https://distill.pub/2017/feature-visualization/) <br/>
  *Olah et al. 2017*
* Understanding Neural Networks Through Deep Visualization [\[paper\]](https://arxiv.org/abs/1506.06579) <br/>
  *Jason Yosinski et al., 2015*
* Striving for Simplicity: The All Convolutional Net [\[paper\]](https://arxiv.org/abs/1412.6806) <br/>
  *Jost Tobias Springenberg et al. 2015*
* Understanding Deep Image Representations by Inverting Them [\[paper\]](https://arxiv.org/pdf/1412.0035.pdf) <br/>
  *Aravindh Mahendran et al., 2015*
* Deep Inside Convolutional Network: Visualising Image Classification Models and Saliency Maps [\[paper\]](https://arxiv.org/pdf/1312.6034.pdf) <br/>
  *Karen Simonyan et al., 2013*
* Visualizing and Understanding Convolutional Networks [\[paper\]](https://arxiv.org/pdf/1311.2901.pdf) <br/>
  *Matthew D Zeiler et al., 2013*

### 1-2. Feature maps: Visualization, Interpretation
* Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space [\[paper\]](https://arxiv.org/abs/1612.00005)
  *Anh Nguyen et al., 2017*
* Inverting Visual Representations with Convolutional Networks [\[paper\]](https://arxiv.org/abs/1506.02753) <br/>
  *Alexey Dosovitskiy et al., 2016*
* Object Detectors Emerge in Deep Scene CNNs [\[paper\]](https://arxiv.org/abs/1412.6856) <br/>
  *Bolei Zhou et al., 2015*

## 2. Diagnosis of CNN representations

### 2-1. Analyzing CNN features
* Understanding Deep Features with Computer-generated Imagery [\[paper\]](https://arxiv.org/abs/1506.01151) <br/>
  *Mathieu Aubry et al., 2015*
* How Transferable are Features in Deep Neural Networks [\[paper\]](https://arxiv.org/abs/1411.1792) <br/>
  *Jason Yosinski et al., 2014* 
* Going Deeper with Convolutions [\[paper\]](https://arxiv.org/abs/1409.4842) <br/>
  *Christian Szegedy et al., 2014*

### 2-2. Extracting image regions from the network output
* Interpretable Explanations of Black Boxes by Meaningful Perturbation [\[paper\]](https://arxiv.org/abs/1704.03296) <br/>
  *Ruth Fong et al., 2017*
* Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization [\[paper\]](https://arxiv.org/abs/1610.02391) <br/>
  *Ramprasaath R. Selvaraju et al., 2017*
* Visualizing Deep Neural Network Decisions: Prediction Difference Analysis [\[paper\]](https://arxiv.org/abs/1702.04595) <br/>
  *Luisa M Zintgraf et al., 2017*
* The (Un)reliability of saliency methods [\[paper\]](https://arxiv.org/abs/1711.00867) <br/>
  *Pieter-Jan Kindermans et al., 2017*
* "Why Should I Trust You?": Explaining the Predictions of Any Classifier [\[paper\]](https://arxiv.org/abs/1602.04938) <br/>
  *Marco Tulio Ribeiro et al., 2016*

### 2-3. Vulnerability of Deep Neural Net Model
* Understanding Block-box Predictions via Influence Functions [\[paper\]](https://arxiv.org/abs/1703.04730) <br/>
  *Pang Wei Koh et al., 2017*
* One Pixel Attack for Fooling Deep Neural Networks [\[paper\]](https://arxiv.org/abs/1710.08864) <br/>
  *Jiawei Su et al., 2017*

### 2-4. Refining Network Representations
* Harnessing Deep Neural Networks with Logic Rules [\[paper\]](https://arxiv.org/abs/1603.06318) <br/>
  *Zhiting Hu et al., 2016*

### 2-5. Discovering Bugs in Neural Networks
* Examing CNN Representations with respect to Dataset Bias [\[paper\]] <br/>
  *Quanshi Zhang et al., 2017*

## 3. Disentangling CNN representations into
* Growing Interpretable Part Graphs on ConvNets via Multi-Shot Learning [\[paper\]](https://arxiv.org/abs/1611.04246) <br/>
  *Quanshi Zhang et al.,2016*
* Interpreting CNN Knowledge Via An Explanatory Graph [\[paper\]](https://arxiv.org/abs/1708.01785) <br/>
  *Quanshi Zhang et al.,2018*
* Interpreting CNNs via Decision Trees [\[paper\]](https://arxiv.org/abs/1802.00121) <br/>
  *Quanshi Zhang et al.,2018*
* Interpret Neural Networks by Identifying Critical Data Routing Paths [\[paper\]](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Interpret_Neural_Networks_CVPR_2018_paper.pdf) <br/>
  *Yulong Wang et al.,2018*

## 4. Building explainable models
Modifying model structure to interpret model

* Interpretable Convolution Neural Networks [\[paper\]](https://arxiv.org/abs/1710.00935) <br/>
  *Quanshi Zhang et al.,2018*
* Towards Interpretable R-CNN by Unfolding Latent Structures [\[paper\]](https://arxiv.org/abs/1711.05226)
  *Tianfu Wu et al.,2018*
* Dynamic Routing Between Capsules [\[paper\]](https://arxiv.org/abs/1710.09829) <br/>
  *Sara Sabour et al.,2017*
* InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets [\[paper\]](https://arxiv.org/abs/1606.03657) <br/>
  *Xi Chen et al.,2016*

## 5. Evaluation metrics for network interpretability

### 5-1. Filter interpretability
* Network Dissection: Quantifying Interpretability of Deep Visual Representations [\[paper\]](https://arxiv.org/abs/1704.05796) <br/>
  *David Bau et al., 2017*

### 5-2. Location instability
* Interpreting CNN Knowledge Via An Explanatory Graph [\[paper\]](https://arxiv.org/abs/1708.01785) <br/>
  *Quanshi Zhang et al.,2018*

## 99. Survey Papers

* Visual Interpretability for Deep Learning: a Survey [\[paper\]](https://arxiv.org/pdf/1802.00614.pdf) <br/>
  *Quanshi Zhang,Song-Chun Zhu*, *2018*


